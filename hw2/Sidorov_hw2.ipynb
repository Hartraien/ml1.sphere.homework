{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2: Линейные модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <hr\\>\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 09 ноября 18:00 Сдача **очная** на онлайн занятии. <br\\>\n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "\n",
    "[ML0920, Задание 2] Фамилия Имя.\n",
    "\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. В противном случае -1 балл\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! \n",
    "\n",
    "В этом задании мы будем реализовать линейные модели. Необходимо реализовать линейную и логистическую регрессии с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическое введение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Не забываем, что здесь и далее  мы считаем, что в $x_i$ есть тождественный вектор единиц, ему соответствует вес $w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует так называемый функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (мы же моделируем вероятность все-таки!)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Если внимательно посмотреть на функцию потерь, то можно заметить, что в зависимости от правильного ответа алгоритм штрафуется или функцией $-\\log a_i$, или функцией $-\\log (1 - a_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Самое понятие регуляризации введено основателем ВМК академиком Тихоновым https://ru.wikipedia.org/wiki/Метод_регуляризации_Тихонова\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу (что это такое можно найти в интернете), для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений. На лекции Вы уже рассмотрели два варианта регуляризации.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "Вам нужно добавить соотвествущую Вашему варианту $L2$ регуляризацию.\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. В этом задании Вам предложат реализовать стохастический градиентный спуск или  мини-батч градиентный спуск (мини-батч на русский язык довольно сложно перевести, многие переводят это как \"пакетный\", но мне не кажется этот перевод удачным). Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (далее один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу (не забыть поделить на  число объектов в мини-батче)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* Для отладки алгоритма реализуйте возможность  вывода средней ошибки на обучении модели по объектам (мини-батчам). После шага градиентного спуска посчитайте значение ошибки на объекте (или мини-батче), а затем усредните, например, по ста шагам. Если обучение проходит корректно, то мы должны увидеть, что каждые 100 шагов функция потерь уменьшается. \n",
    "* Правило останова - максимальное количество эпох\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические вопросы (2 балла)\n",
    "В этой части Вам будут предложены теоретичские вопросы и задачи по теме. Вы, конечно, можете списать их у своего товарища или найти решение в интернете, но учтите, что они обязательно войдут в теоретический коллоквиум. Лучше разобраться в теме сейчас и успешно ответить на коллоквиуме, чем списать, не разобравшись в материале, и быть терзаемым совестью. \n",
    "\n",
    "\n",
    "Формулы надо оформлять в формате **LaTeX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 1. Градиент для линейной регрессии.\n",
    "* Выпишите формулу обновления весов для линейной регрессии с L2 регуляризацией для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\sum_Dw^2 $$\n",
    "\n",
    "$$\\nabla L(w) = -\\frac{2}{n}\\left[\\sum_i x_i(y_i - (w, x_i)) \\right] + \\frac{2}{C}w $$\n",
    "\n",
    "$$\\nabla L(w) = \\frac{2}{n}\\left[\\sum_i x_i((w, x_i)-y_i) \\right] + \\frac{2}{C}w $$\n",
    "\n",
    "$$ w_{new} = w_{old} - \\alpha \\cdot (\\frac{2}{n}\\sum_{i=1}^{n}(x_i((w_{old}, x_i) - y_i))+\\frac{2}{C}w_{old})$$\n",
    "\n",
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2. Градиент для логистической регрессии.\n",
    "* Выпишите формулу обновления весов для логистической регрессии с L2 регуляризацией  для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент? Как соотносится этот градиент с градиентом, возникающий в задаче линейной регрессии?\n",
    "\n",
    "Подсказка: Вам градиент, которой получается если “в лоб” продифференцировать,  надо немного преобразовать.\n",
    "Надо подставить, что $1 - \\sigma(w,x) $ это  $1 - a(x_i)$, а  $-\\sigma(w,x)$ это $0 - a(x_i)$.  Тогда получится свести к одной красивой формуле с линейной регрессией, которую программировать будет намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "$$f(w) = y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i)$$\n",
    "\n",
    "$$\\nabla f(w) = \\frac{y_i}{a_i}a'_i - \\frac{1-y_i}{1-a_i}a'_i = a'_i(\\frac{y_i}{a_i} - \\frac{1-y_i}{1-a_i}) = a'_i\\frac{y_i - a_iy_i - a_i + a_iy_i}{(1-a_i)a_i} = a'_i\\frac{y_i - a_i}{(1-a_i)a_i}$$\n",
    "\n",
    "$$a'_i(w) = \\nabla \\sigma(<w,x>) = \\sigma(<w,x>)(1-\\sigma(<w,x>))x = a_i(1-a_i)x_i$$\n",
    "\n",
    "$$\\nabla f(w) = (y_i - a_i)x_i$$\n",
    "\n",
    "$$\\nabla L(w) = -\\frac{1}{n}\\sum_{i=1}^{n}(x_i(y_i - \\sigma(w, x_i)))+\\frac{2}{C}w$$\n",
    "\n",
    "$$\\nabla L(w) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i(\\sigma(w, x_i) - y_i))+\\frac{2}{C}w$$\n",
    "\n",
    "$$ w_{new} = w_{old} - \\alpha \\cdot (\\frac{1}{n}\\sum_{i=1}^{n}(x_i(\\sigma(w_{old}, x_i) - y_i))+\\frac{2}{C}w_{old})$$\n",
    "\n",
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 3. Точное решение линейной регрессии\n",
    "\n",
    "На лекции было показано, что точное решение линейной регрессии имеет вид $w = (X^TX)^{-1}X^TY $. \n",
    "* Покажите, что это действительно является точкой минимума в случае, если матрица X имеет строк не меньше, чем столбцов и имеет полный ранг. Подсказка: посчитайте Гессиан и покажите, что в этом случае он положительно определен. \n",
    "* Выпишите точное решение для модели с $L2$ регуляризацией. Как L2 регуляризация помогает с точным решением где матрица X имеет линейно зависимые признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "Первая часть:\n",
    "$$\\nabla_w Q(w) = 2X^TXw - 2X^Ty$$\n",
    "$$H_q(w) = \\nabla^2_w Q(w) = \\nabla_w 2X^TXw = 2X^TX$$\n",
    "Нужно доказать, что $H_q(w)$ - положительно определённая квадратичная форма, то есть нужно показать, что\n",
    "$$z^t2X^TXz >0 \\forall z \\neq 0$$\n",
    "$$z^t2X^TXz = 2z^TX^TXz = 2(Xz)^T(Xz) = 2||Xz||^2 \\geq 0$$\n",
    "$$||Xz||^2 = 0 \\iff z = 0 \\text{ или в $X$ есть линейно зависимые столбцы}$$\n",
    "Из того, что в $X$ строк не меньше, чем столбцов ($rows \\geq cols$)  матрица $X$ имеет полный ранг $rank(X) = cols$, следует, что требуемое доказано и Гессиан - положительно определён.\n",
    "Отсюда следует, то $w = (X^TX)^{-1}X^TY $ - точка минимума функции $Q(w)$\n",
    "\n",
    "Вторая часть:\n",
    "\n",
    "В модели с L2 регуляризацией в $Q(w)$ добавляется слагаемое вида $R(w)\\frac{1}{C}\\sum_D{w^2} = \\frac{1}{C}w^Tw$ (C - коэффициент регуляризации)\n",
    "$$\\nabla_w R(w) = \\frac{1}{C}\\nabla_w (w^Tw) = \\frac{1}{C}\\nabla_w (w^TEw) = \\frac{1}{C} (E + E^T)w =\\frac{2}{C}Ew$$\n",
    "\n",
    "$$\\nabla Q*(w) = \\nabla Q(w) + \\nabla R(w) = 2X^TXw - 2X^Ty + \\frac{2}{C}Ew$$\n",
    "Откуда мы получаем точное решение для модели с L2 регуляризацией: \n",
    "$$ w = (X^TX + \\frac{2}{C}E)^{-1}X^Ty$$\n",
    "\n",
    "Как L2 регуляризация помогает с точным решением в случае, когда в X есть линейно зависимые столбцы (признаки):\n",
    "\n",
    "Из первого пункта мы получили, что $X^TX$ - положительно полуопределённая матрица, то есть у неё все собственные значения неотрицательны. В случае, если есть хотя бы одно собственное значение равное 0, то матрица $X^TX$ получается необратимой и точное решение получить невозможно. Из того, что $X^TX$ - положительно полуопределённая матрица следует, что она представима в следующем виде:\n",
    "$$X^TX = V{\\Lambda}V^{-1}$$\n",
    "Где $V$ - матрица, столбцы которой - собственные векторы $X^TX$, а $\\Lambda$ - диагональная матрица из собственных значений матрицы $X^TX$. Тогда\n",
    "$$X^TX+\\frac{2}{C}E = V{\\Lambda}V^{-1} + \\frac{2}{C}VV^{-1} = V{\\Lambda}V^{-1} + V\\frac{2}{C}EV^{-1} = V({\\Lambda}+\\frac{2}{C}E)V^{-1}$$\n",
    "То есть все собственные значения матрицы $X^TX+\\frac{2}{C}E$ равны $\\lambda_i +\\frac{2}{C} \\geq \\frac{2}{C} > 0$, то есть матрица $X^TX+\\frac{2}{C}E$ положительно определена, из чего следует её обратимость.\n",
    "\n",
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 4.  Предсказываем вероятности.\n",
    "\n",
    "Когда говорят о логистической регрессии, произносят фразу, что она \"предсказывает вероятности положительного класса\". Давайте разберемся, что же за этим стоит. Посчитаем математическое ожидание функции потерь и проверим, что предсказание алгоритма, оптимизирующее это мат. ожидание, будет являться вероятностью положительного класса. \n",
    "\n",
    "И так, функция потерь на объекте $x_i$, который имеет метку $y_i \\in \\{0,1\\}$  для предсказания $a(x_i)$ равна:\n",
    "$$L(y_i, b) =-[y_i == 1] \\log a(x_i)  - [y_i == 0] \\log(1 - a(x_i)) $$\n",
    "\n",
    "Где $[]$ означает индикатор $-$ он равен единице, если значение внутри него истинно, иначе он равен нулю. Тогда мат. ожидание при условии конкретного $x_i$  по определение мат. ожидания дискретной случайной величины:\n",
    "$$E(L | x_i) = -p(y_i = 1 |x_i ) \\log a(x_i)  - p(y_i = 0 | x_i) \\log( 1 - a(x_i))$$\n",
    "* Докажите, что значение $a(x_i)$, минимизирующее данное мат. ожидание, в точности равно $p(y_i = 1 |x_i)$, то есть равно вероятности положительного класса.\n",
    "\n",
    "Подсказка: возможно, придется воспользоваться, что  $p(y_i = 1 | x_i) + p(y_i = 0 | x_i) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "$$ p(y_i = 1|x_i) = p $$\n",
    "$$ p(y_i = 0|x_i) = 1-p $$\n",
    "\n",
    "$$E'(a) = -\\frac{p}{a}+\\frac{1-p}{1-a} = \\frac{a-ap-p+ap}{(1-a)a} = \\frac{a-p}{(1-a)a}$$\n",
    "То есть $a = p$ - точка экстремума.\n",
    "Так как $a = \\sigma(<w,x>) \\in (0,1)$, то $(1-a)a$>0.\n",
    "При $a>p, E'(a)>0$, при $a<p E'(a)<0$ $\\Rightarrow$ $a=p$ - точка минимума\n",
    "\n",
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 5.  Смысл регуляризации.\n",
    "\n",
    "Нужно ли в L1/L2 регуляризации использовать свободный член $w_0$ (который не умножается ни на какой признак)?\n",
    "\n",
    "Подсказка: подумайте, для чего мы вводим $w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "Нет, w_0 не нужно учитывать в регуляризации.\n",
    "1) Мы вводим $w_0$ для того, чтобы (в геометрических терминах) наша разделяющая гиперплоскость могла проходить не через начало координат. Если мы в слагаемом регуляризации в градиенте учитываем ещё и $w_0$, то мы устремляем $w_0$ к 0, что противоположно цели $w_0$.\n",
    "\n",
    "2) Так как $w_0$ - вес соответствующий фиктивному признаку x_0 = 1, который не кореллирует ни с одним другим признаком, а регуляризация нужна, в том числе, и для \"борьбы\" с линейно зависимыми признаками, нет нужды регуляризировать с учётом $w_0$.\n",
    "\n",
    "***Ваше решение здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Реализация линейной модели (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны батчи?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы могли заметить из теоретического введения, что в случае SGD, что в случа mini-batch GD,  на каждой итерации обновление весов  происходит только по небольшой части данных (1 пример в случае SGD, batch примеров в случае mini-batch). То есть для каждой итерации нам *** не нужна вся выборка***. Мы можем просто итерироваться по выборке, беря батч нужного размера (далее 1 объект тоже будем называть батчом).\n",
    "\n",
    "Легко заметить, что в этом случае нам не нужно загружать все данные в оперативную память, достаточно просто считать батч с диска, обновить веса, считать диска другой батч и так далее. В целях упрощения домашней работы, прямо с диска  мы считывать не будем, будем работать с обычными numpy array. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немножко про генераторы в Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея считывания данных кусками удачно ложится на так называемые ***генераторы*** из языка Python. В данной работе Вам предлагается не только разобраться с логистической регрессией, но  и познакомиться с таким важным элементом языка.  При желании Вы можете убрать весь код, связанный с генераторами, и реализовать логистическую регрессию и без них, ***штрафоваться это никак не будет***. Главное, чтобы сама модель была реализована правильно, и все пункты были выполнены. \n",
    "\n",
    "Подробнее можно почитать вот тут https://anandology.com/python-practice-book/iterators.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К генератору стоит относиться просто как к функции, которая порождает не один объект, а целую последовательность объектов. Новое значение из последовательности генерируется с помощью ключевого слова ***yield***. Ниже Вы можете насладиться  генератором чисел Фибоначчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(max_iter=4):\n",
    "    a, b = 0, 1\n",
    "    iter_num = 0\n",
    "    while 1:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно сгенерировать последовательность Фибоначчи. \n",
    "\n",
    "Заметьте, что к генераторам можно применять некоторые стандартные функции из Python, например enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for j, fib_val in enumerate(new_generator):\n",
    "    print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересоздавая объект, можно сколько угодно раз генерировать заново последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    new_generator = fib()\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так уже нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for i in range(0, 3):\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция крайне удобная для обучения  моделей $-$ у Вас есть некий источник данных, который Вам выдает их кусками, и Вам совершенно все равно откуда он их берет. Под ним может скрывать как массив в оперативной памяти, как файл на жестком диске, так и SQL база данных. Вы сами данные никуда не сохраняете, оперативную память экономите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Вам понравилась идея с генераторами, то Вы можете реализовать свой, используя прототип batch_generator. В нем Вам нужно выдавать батчи признаков и ответов для каждой новой итерации спуска. Если не понравилась идея, то можете реализовывать SGD или mini-batch GD без генераторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "            \n",
    "    if batch_size>X.shape[0]:\n",
    "        batch_size = X.shape[0]\n",
    "    indices = np.arange(len(X))\n",
    "    if shuffle==True:\n",
    "        np.random.shuffle(indices)\n",
    "    for i in indices[:X.shape[0]-batch_size+1]:\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        yield (X_batch, y_batch)\n",
    "\n",
    "# Теперь можно сделать генератор по данным ()\n",
    "#  my_batch_generator = batch_generator(X, y, shuffle=True, batch_size=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    \n",
    "    sigm_value_x = 1.0 / (1.0 + np.exp(-1 * x))\n",
    "    return sigm_value_x\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.k = 1\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter': [], 'loss': []}\n",
    "        self.weights_log = []\n",
    "        self.model_type = model_type\n",
    "        self.test = True\n",
    "\n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        if self.model_type == 'lin_reg':\n",
    "            error = self.calcPredict(X_batch) - y_batch\n",
    "            loss = np.sum(error ** 2)/(X_batch.shape[0])\n",
    "        elif self.model_type == 'log_reg':\n",
    "            y_pred = self.calcPredict(X_batch)\n",
    "            loss = -np.mean((y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred)))\n",
    "        #loss = loss + self.l2()\n",
    "        return loss\n",
    "\n",
    "    def calcPredict(self, X):\n",
    "        \"\"\"\n",
    "        X - батч или вся матрица признаков\n",
    "        ---\n",
    "        output:\n",
    "        a(X) - предсказание алгоритма, равное\n",
    "        np.dot(X, self.weights) для линейной регрессии\n",
    "        sigmoid(np.dot(X, self.weights)) для логистической регрессиии\n",
    "        \"\"\"\n",
    "        res = np.dot(X, self.weights)\n",
    "        if self.model_type == 'lin_reg':\n",
    "            return res\n",
    "        elif self.model_type == 'log_reg':\n",
    "            return sigmoid(res)\n",
    "        return \"test\"\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        loss_grad = 0\n",
    "        error = self.calcPredict(X_batch) - y_batch\n",
    "        if self.model_type == 'lin_reg':\n",
    "            loss_grad = 2 * np.dot(error, X_batch)\n",
    "        elif self.model_type == 'log_reg':\n",
    "            loss_grad = np.dot(error, X_batch)\n",
    "        loss_grad = loss_grad / (X_batch.shape[0])\n",
    "        #loss_grad += self.l2_der()\n",
    "        return loss_grad\n",
    "\n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        self.weights -= self.alpha * new_grad\n",
    "        return\n",
    "\n",
    "    def fit(self, X, y, batch_size_arg=1):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "\n",
    "        # Нужно инициализровать случайно веса\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "        self.weights = np.clip(np.random.randn(X.shape[1]),-1,1)\n",
    "        self.weights_log.append(self.weights)\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(X, y, batch_size=batch_size_arg, shuffle=False)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                # Подумайте в каком месте стоит посчитать ошибку для отладки модели\n",
    "                # До градиентного шага или после\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "                self.weights_log.append(self.weights)\n",
    "        return self\n",
    "\n",
    "    def l2(self):\n",
    "        \"\"\"\n",
    "        Возвращает значение R(w) для L2 регуляризации\n",
    "        \"\"\"\n",
    "        #return np.sum(self.weights[1:] ** 2) / self.C\n",
    "        return 0\n",
    "\n",
    "    def l2_der(self):\n",
    "        \"\"\"\n",
    "        Возвращает значение производной от R(w) для L2 регуляризации\n",
    "        Используется в вычислении градиента\n",
    "        \"\"\"\n",
    "        return 0\n",
    "        res = (2.0 / self.C) * self.weights\n",
    "        res[0] = 0\n",
    "        return res\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "        if self.model_type == 'lin_reg':\n",
    "            y_hat = -np.sign(self.calcPredict(X))\n",
    "            y_hat[y_hat==-1]=0\n",
    "        elif self.model_type == 'log_reg':\n",
    "            y_hat = np.round(self.calcPredict(X))\n",
    "        # Желательно здесь использовать матричные операции между X и весами, например, numpy.dot\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.asarray([ 1.   ,       0.78580383, -0.05746952])\n",
    "X_batch = np.asarray([[ 1.     ,     5.10023581, -2.69111611],[ 1.      ,    7.8613398,  -0.59027583]])\n",
    "print(weights)\n",
    "print(X_batch)\n",
    "res = np.dot(X_batch, weights)\n",
    "print (res)\n",
    "print(sigmoid(res[0]),sigmoid(res[1]))\n",
    "sigmoid(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите обе регрессии на синтетических данных. \n",
    "\n",
    "\n",
    "Выведите полученные веса и нарисуйте разделяющую границу между классами (используйте только первых два веса для первых двух признаков X[:,0], X[:,1] для отображения в 2d пространство ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf):\n",
    "    weights = clf.weights[:3]\n",
    "    #weights = [-46, 8.6, -7.8]\n",
    "    k = 0\n",
    "    x = np.linspace(-2,6,50)\n",
    "    y = (k-weights[0] - weights[1]*x)/weights[2]\n",
    "    plt.plot(x,y)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.149478623540693, 2.5541609229694955, 1.351961747094406, 0.7945749200708337, 0.5356489557600276, 0.4148757897905262, 0.35805696764287165, 0.33084961086993103, 0.317357889457825, 0.310224153894749]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "clf_test = MySGDClassifier(batch_generator=batch_generator, model_type = 'lin_reg', alpha=0.01).fit(X,y, batch_size_arg=X.shape[0])\n",
    "\n",
    "loss = clf_test.errors_log['loss']\n",
    "plt.plot(np.arange(len(loss)), loss)\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "clf_log = MySGDClassifier(batch_generator=batch_generator, model_type = 'log_reg').fit(X,y, batch_size_arg=1)\n",
    "clf_lin = MySGDClassifier(batch_generator=batch_generator, model_type = 'lin_reg').fit(X,y, batch_size_arg=1)\n",
    "\n",
    "plot_decision_boundary(clf_log)\n",
    "plot_decision_boundary(clf_lin)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clf.errors_log['loss'][:10])\n",
    "#print(len(clf.errors_log['loss']))\n",
    "step = clf_log.max_epoch\n",
    "chunks = [clf_log.errors_log['loss'][x:x+step] for x in range(0, len(clf_log.errors_log['loss']), step)]\n",
    "#print(chunks[:10])\n",
    "chunks = np.asarray(chunks)\n",
    "#print(chunks[:1])\n",
    "chunks = chunks.mean(axis = 1)\n",
    "plt.plot(np.arange(len(chunks)), chunks, '-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clf.errors_log['loss'][:10])\n",
    "#print(len(clf.errors_log['loss']))\n",
    "step = clf_lin.max_epoch\n",
    "chunks = [clf_lin.errors_log['loss'][x:x+step] for x in range(0, len(clf_lin.errors_log['loss']), step)]\n",
    "#print(chunks[:10])\n",
    "chunks = np.asarray(chunks)\n",
    "#print(chunks[:1])\n",
    "chunks = chunks.mean(axis = 1)\n",
    "plt.plot(np.arange(len(chunks)), chunks, '-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем анализировать Ваш алгоритм. \n",
    "Для этих заданий используйте датасет ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100000, n_features=10, \n",
    "                           n_informative=4, n_redundant=0, \n",
    "                           random_state=123, class_sep=1.0,\n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите сходимости обеих регрессией на этом датасете: изобразите график  функции потерь, усредненной по $N$ шагам градиентого спуска, для разных `alpha` (размеров шага). Разные `alpha` расположите на одном графике. \n",
    "\n",
    "$N$ можно брать 10, 50, 100 и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAverageLossForAlpha(X,y, model_type = 'lin_reg', N = 100, alpha = 0.01):\n",
    "    clf= MySGDClassifier(batch_generator = batch_generator, alpha=alpha, model_type=model_type).fit(X,y, batch_size_arg=1)\n",
    "    chunks = np.asarray([np.mean(np.asarray(clf.errors_log['loss'][x:x+N])) for x in range(0, len(clf_lin.errors_log['loss']), N)])\n",
    "    chunks = chunks\n",
    "    plt.plot(np.arange(len(chunks)), chunks, '-o')\n",
    "    return\n",
    "\n",
    "alpha_array = [0.05,0.01,0.005,0.001]\n",
    "for alpha in alpha_array[:1]:\n",
    "    printAverageLossForAlpha(X,y,model_type='lin_reg', alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_array = [0.05,0.01,0.005,0.001]\n",
    "for alpha in alpha_array:\n",
    "    printAverageLossForAlpha(X,y,model_type='log_reg', alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что Вы можете сказать про сходимость метода при различных `alpha`? Какое значение стоит выбирать для лучшей сходимости?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите график среднего значения весов для обеих регрессий в зависимости от коеф. регуляризации С из `np.logspace(3, -3, 10)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightFromC(X,y, model_type = 'lin_reg', alpha=0.05):\n",
    "    weight_array = []\n",
    "    for C in np.logspace(3,-3,10):\n",
    "        clf= MySGDClassifier(batch_generator = batch_generator, alpha=alpha, model_type=model_type, C = C).fit(X,y, batch_size_arg=1)\n",
    "        weight_array.append(np.mean(clf.weights[1:]))\n",
    "    plt.plot(range(len(weight_array)),weight_array, '-o')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightFromC(X,y,model_type='lin_reg', alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightFromC(X,y,model_type='log_reg', alpha = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольны ли Вы, насколько сильно уменьшились Ваши веса? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (4  балла)\n",
    "\n",
    "**Защита данной части возможна только при преодолении в проекте бейзлайна Handmade baseline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим модель на итоговом проекте! Датасет сделаем точно таким же образом, как было показано в project_overview.ipynb\n",
    "\n",
    "Применим обе регрессии, подберем для них параметры и сравним качество. Может быть Вы еще одновременно с решением домашней работы подрастете на лидерборде!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию. Подберите параметры C, alpha, max_epoch, model_type на валидации (Вы же помните, как правильно в этой задаче делать валидацию?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Подберите порог линейной модели, по достижении которого, Вы будете относить объект к классу 1. Вспомните, какую метрику мы оптимизируем в соревновании.  Как тогда правильно подобрать порог?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** При сдаче домашки Вам необходимо кроме ссылки на ноутбук показать Ваш ник на kaggle, под которым Вы залили решение, которое побило Handmade baseline. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения линейных моделей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ВАШ ОТЗЫВ ЗДЕСЬ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
